---
layout: post
title: "Robots.txt and Humans.txt"
date: 2018-11-15 14:45:00 +0100
categories: blog
author: Mikael Eriksson
---
I will here briefly comment on the usage of robots.txt and humans.txt files. I have never before used theese kinds of files so consider this to be comments on the subject from the perspective of a beginner.

## What are they?
First, I will decribe the basics of both file types, what they do and why you would use them. If you are already familiar with the concepts, feel free to skip ahead to the next section.

### Robots.txt
A robots.txt file is used by the owner of a web site as a way to communicate with Web robots (which are also called Web Wanderers, Crawlers, or Spiders). Web robots travel across the web automatically and are used by search engines to index the content of the web. They can also be used by spammers to collect e-mail adresses or by social media web sites such as Facebook, to continuously update their users' feed.

When a Web robot visits a web site, it will firstly look for a robots.txt file in order to find instructions. The instructions in this file will tell the robot what it is allowed to look at, by telling the robot what it is *not allowed* to look at. Of course, web robots designed by spammers can simply chose to ignore the robots.txt file, which is why a robots.txt file should not be used as a means to hide information.

### Humans.txt
The humans.txt is a completely voluntary file which is used to tell human visitors a bit about the people behind a web site. 